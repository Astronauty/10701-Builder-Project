{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcf179ab-6361-4c41-a7f8-44b9b894ddf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the dataframe and storing untokenized pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 94292.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding sentences to Langs amd geting data pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2125.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tokenized pairs of english and french sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 6281.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from DatasetSize import DatasetSize\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformer import TransformerMT\n",
    "from torch import optim\n",
    "from TrainEval import TrainEval\n",
    "from DataLoaderProvider import DataLoaderProvider\n",
    "import pickle\n",
    "from torchtext.data import get_tokenizer\n",
    "from data_loader import Langs\n",
    "\n",
    "en_tokenizer = get_tokenizer(tokenizer='spacy', language='en_core_web_sm')\n",
    "fr_tokenizer = get_tokenizer(tokenizer='spacy', language='fr_core_news_sm')\n",
    "\n",
    "dataset_size = DatasetSize.MEDIUM\n",
    "\n",
    "max_sequence_length = 50\n",
    "dataloader_provider = DataLoaderProvider(dataset_size=dataset_size, \n",
    "                                         batch_size=256,\n",
    "                                         max_sequence_length=max_sequence_length)\n",
    "\n",
    "en_vocab_path = f\"data/{dataset_size.value}/vocabs/en_vocab.pkl\"\n",
    "fr_vocab_path = f\"data/{dataset_size.value}/vocabs/fr_vocab.pkl\"\n",
    "\n",
    "with open(en_vocab_path, 'rb') as file:\n",
    "    en_vocab : Langs = pickle.load(file)\n",
    "with open(fr_vocab_path, 'rb') as file:\n",
    "    fr_vocab : Langs = pickle.load(file)\n",
    "        \n",
    "model = transformer_mt = TransformerMT(\n",
    "    source_vocabulary_size=en_vocab.n_words,\n",
    "    target_vocabulary_size=fr_vocab.n_words,\n",
    "    embedding_size=512,\n",
    "    max_num_embeddings=max_sequence_length,\n",
    "    num_attention_heads=8,\n",
    "    num_encoder_layers=4,\n",
    "    num_decoder_layers=4,\n",
    "    linear_layer_size=1024,\n",
    "    dropout=0.1,\n",
    "    activation='relu',\n",
    "    layer_norm_eps=1e-5,\n",
    "    batch_first=True,\n",
    "    norm_first=False,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "train_eval = TrainEval(\n",
    "    dataloader_provider=dataloader_provider,\n",
    "    num_epochs=50,\n",
    "    optimizer=optim.Adam(transformer_mt.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9),\n",
    "    loss_function=CrossEntropyLoss(ignore_index=0),\n",
    "    model=model,\n",
    "    model_shortname='transformer',\n",
    "    disambiguator='probe3'\n",
    ")\n",
    "\n",
    "train_eval.execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
