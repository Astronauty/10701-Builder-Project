{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aipex6/anaconda3/envs/conan_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import copy\n",
    "import spacy\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "from funnel_transformer_conan import *\n",
    "from data_loader import *\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/funnel_experiment2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split(or abridged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class split_csv():\n",
    "    def __init__(self, Generate_train_test_split:bool, create_abridged: bool, abridged_size: int):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            abridged (bool): Use the generated abridged dataset \n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        self.Generate_train_test_split = Generate_train_test_split\n",
    "        self.create_abridged = create_abridged\n",
    "        self.abridged_size = abridged_size\n",
    "        self.full_dataset_path = Path(\"data/en-fr.csv\")\n",
    "        self.abridged_dataset_path = Path(\"data/en-fr-training.csv\")\n",
    "        \n",
    "        self.en_tokenizer = get_tokenizer(tokenizer='spacy',language='en_core_web_sm')\n",
    "        self.fr_tokenizer = get_tokenizer(tokenizer='spacy',language='fr_core_news_sm')\n",
    "        \n",
    "        self.process()\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "         # Create abridged dataset if it doesnt exist and load either full or abridged data into self.ds \n",
    "        # full_dataset_path = 'data/en-fr.csv'\n",
    "        # abridged_dataset_path = 'data/en-fr-abridged.csv'\n",
    "        self.full_dataset_path.parent.mkdir(parents=True, exist_ok=True) # make datafolder if it doesn't exist\n",
    "        \n",
    "        # Check if the full dataset exists\n",
    "        if not self.full_dataset_path.exists():\n",
    "            raise FileNotFoundError(\"The full dataset does not exist. Please download it from https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset/data and place in the /data folder.\")\n",
    "        \n",
    "        # Create the abridged dataset if it does not exist\n",
    "        if  self.Generate_train_test_split:      \n",
    "            print(\"Creating train test dataset csvs...\")\n",
    "            print(\"reading full dataset...\")\n",
    "            full_dataset = pd.read_csv(self.full_dataset_path,encoding=\"utf-8\", keep_default_na=False)\n",
    "            if self.create_abridged:\n",
    "                print(\"splitting abridged\")\n",
    "                abridged_dataset = full_dataset.head(self.abridged_size)\n",
    "                print(\"splitting abridged train-test\")\n",
    "                abridged_dataset_train = abridged_dataset.head(int(self.abridged_size*0.9))\n",
    "                abridged_dataset_val = abridged_dataset.tail(int(self.abridged_size*0.1))\n",
    "                abridged_dataset_train.to_csv(\"data/en-fr-abridged-train.csv\", index=False)\n",
    "                abridged_dataset_val.to_csv(\"data/en-fr-abridged-val.csv\", index=False)\n",
    "            else:\n",
    "                print(\"splitting fulldata train-test\")\n",
    "                training_len = int(len(full_dataset)*0.8)\n",
    "                testing_len = int(len(full_dataset)-len(full_dataset)*0.9)\n",
    "\n",
    "                abridged_dataset_training = full_dataset[:training_len]\n",
    "                abridged_dataset_validation = full_dataset[training_len:training_len+testing_len]\n",
    "                abridged_dataset_testing = full_dataset[training_len+testing_len:]\n",
    "                del full_dataset\n",
    "\n",
    "                print(\"Creating full data csvs...\")\n",
    "                abridged_dataset_training.to_csv(\"data/en-fr-train.csv\", index=False)\n",
    "                abridged_dataset_validation.to_csv(\"data/en-fr-val.csv\", index=False)\n",
    "                abridged_dataset_testing.to_csv(\"data/en-fr-test.csv\", index=False)\n",
    "            \n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train test dataset csvs...\n",
      "reading full dataset...\n",
      "splitting abridged\n",
      "splitting abridged train-test\n"
     ]
    }
   ],
   "source": [
    "_ = split_csv(Generate_train_test_split=True,create_abridged=True, abridged_size=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the sequence distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = get_tokenizer(tokenizer='spacy',language='en_core_web_sm')\n",
    "fr_tokenizer = get_tokenizer(tokenizer='spacy',language='fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_sequence_distribution(ds,en_tokenizer,fr_tokenizer):\n",
    "    en_seq_len_list = []\n",
    "    fr_seq_len_list = []\n",
    "    for i in tqdm(range(len(ds))):\n",
    "        en_seq_len_list.append(len(en_tokenizer(ds['en'][i].lower())))\n",
    "        fr_seq_len_list.append(len(fr_tokenizer(ds['fr'][i].lower())))\n",
    "\n",
    "    return en_seq_len_list, en_seq_len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22520376/22520376 [30:05<00:00, 12470.75it/s]\n"
     ]
    }
   ],
   "source": [
    "ds = pd.read_csv(\"data/en-fr.csv\",encoding=\"utf-8\", keep_default_na=False)\n",
    "en_seq_list, fr_seq_list = get_the_sequence_distribution(ds,en_tokenizer, fr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "en_seq_list = np.array(en_seq_list)\n",
    "fr_seq_list = np.array(fr_seq_list)\n",
    "bins = np.arange(0,110,10)\n",
    "fig, ax = plt.subplots(1,2,figsize=(9, 5))\n",
    "ax[1].hist([np.clip(en_seq_list, 0,100), np.clip(fr_seq_list, 0, 100)],cumulative=True,\n",
    "                            bins=bins, color=['#3782CC', '#AFD5FA'], weights=[np.full(len(en_seq_list),100)/len(en_seq_list), np.full(len(en_seq_list),100) / len(en_seq_list)], label=['English', 'French'])\n",
    "xlabels = bins[1:].astype(int).astype(str)\n",
    "xlabels[-1] += '+'\n",
    "\n",
    "N_labels = len(xlabels)\n",
    "ax[1].set_xlim([0, 100])\n",
    "ax[1].set_xticks(10 * np.arange(N_labels))\n",
    "ax[1].set_xticklabels(xlabels)\n",
    "ax[1].set_ylabel(\"Percentage(%)\")\n",
    "ax[1].set_xlabel(\"Sequence Length\")\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"CDF\")\n",
    "fig.tight_layout()\n",
    "\n",
    "ax[0].hist([np.clip(en_seq_list, 0,100), np.clip(fr_seq_list, 0, 100)],\n",
    "                            bins=bins, color=['#3782CC', '#AFD5FA'], weights=[np.full(len(en_seq_list),100)/len(en_seq_list), np.full(len(en_seq_list),100) / len(en_seq_list)], label=['English', 'French'])\n",
    "xlabels = bins[1:].astype(int).astype(str)\n",
    "xlabels[-1] += '+'\n",
    "\n",
    "N_labels = len(xlabels)\n",
    "ax[0].set_xlim([0, 100])\n",
    "ax[0].set_xticks(10 * np.arange(N_labels))\n",
    "ax[0].set_xticklabels(xlabels)\n",
    "ax[0].set_ylabel(\"Percentage(%)\")\n",
    "ax[0].set_xlabel(\"Sequence Length\")\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"PDF\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_greater_50 = (en_seq_list>0).sum()\n",
    "frequency_greater_50/len(en_seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_en = np.median(en_seq_list)\n",
    "mean_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dictionaries and tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_organizer(lang):\n",
    "    vocabs = list(lang.word2index.keys())\n",
    "    for keys in vocabs:\n",
    "        # check if the number of occurence in that key is small\n",
    "        if lang.word2count[keys]<10:\n",
    "            # the vocab keys appeared less than 10 times\n",
    "            index = lang.word2index[keys]\n",
    "            if index > 35:\n",
    "                # remove all the stuff in the dictionaries and change the n_words\n",
    "                del lang.word2index[keys]\n",
    "                del lang.index2word[index]\n",
    "                del lang.word2count[keys]\n",
    "                lang.n_words -= 1\n",
    "                \n",
    "    vocabs = list(lang.index2word.values())\n",
    "    index = [*range(len(vocabs))]\n",
    "    word2index1 = {}\n",
    "    index2word1 = {}\n",
    "    i = 0\n",
    "    for words in vocabs:\n",
    "        if words not in word2index1:\n",
    "            word2index1[words] = i\n",
    "            index2word1[i] = words\n",
    "            i += 1\n",
    "    lang.word2index = word2index1\n",
    "    lang.index2word1 = index2word1\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lang(ds, eng_str=\"en\", fr_str=\"fr\"):\n",
    "    # return if the dataformat is wrong\n",
    "    if type(ds) != pd.core.frame.DataFrame:\n",
    "        raise TypeError(\"Wrong dataframe format!\")\n",
    "        \n",
    "    print(\"Reading the dataframe and storing untokenized pairs...\")\n",
    "    pairs = [(ds[eng_str][i], ds[fr_str][i]) for i in tqdm(range(len(ds)))]\n",
    "    \n",
    "    # create the class objects of Langs for English and French to count the \n",
    "    eng_lang = Langs(\"en\")\n",
    "    fr_lang = Langs(\"fr\")\n",
    "    return eng_lang, fr_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_token_list(sequence, lang):\n",
    "        \"\"\"Tokenize a sequence string in the given english/french language and return the list of tokens.\n",
    "\n",
    "        Args:\n",
    "            sequence (string): _description_\n",
    "            lang (_type_): _description_\n",
    "            en_tokenizer (_type_): _description_\n",
    "            fr_tokenizer (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        max_seq_length = 1000\n",
    "        token_list = []\n",
    "        if lang.lang == \"en\":\n",
    "            words = en_tokenizer(sequence.lower())\n",
    "        else:\n",
    "            words = fr_tokenizer(sequence.lower())\n",
    "            \n",
    "        # truncate the word list if it exceeds the max allowed sequence length\n",
    "        words = words[:max_seq_length - 2] # -2 is to account for the appended SOS and EOS token\n",
    "        \n",
    "        token_list.append(CustomTokens.SOS.value)\n",
    "        for word in words:\n",
    "            if word in lang.word2index:\n",
    "                token_list.append(lang.word2index[word])\n",
    "            else:\n",
    "                token_list.append(CustomTokens.UNK.value)\n",
    "        \n",
    "        token_list.append(CustomTokens.EOS.value)\n",
    "        \n",
    "        # # pad the remainder of the token list \n",
    "        # while len(token_list) < max_seq_length:\n",
    "        #     token_list.append(CustomTokens.PAD.value)\n",
    "        \n",
    "        return token_list\n",
    "\n",
    "def string_data_to_tokens(data, en_lang, fr_lang, filename):\n",
    "    \"\"\"Create tokenized pairs of english and french sentences\n",
    "\n",
    "    Args:\n",
    "        data (_type_): Dictionary of english and french sentences\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    tokenized_data = []\n",
    "    fr_string = \"_fr.csv\"\n",
    "    en_string = \"_en.csv\"\n",
    "    print(\"Creating tokenized pairs of english and french sentences...\")\n",
    "    \n",
    "    with open(filename+en_string, 'w') as csvfile1, open(filename+fr_string, 'w') as csvfile2:  \n",
    "        # creating a csv writer object  \n",
    "        csvwriter1 = csv.writer(csvfile1)  \n",
    "        csvwriter2 = csv.writer(csvfile2)\n",
    "        for i in tqdm(range(len(data))):\n",
    "        # writing the fields  \n",
    "            csvwriter1.writerow(string_to_token_list(data[i][0], en_lang))  \n",
    "            csvwriter2.writerow(string_to_token_list(data[i][1], fr_lang))  \n",
    "\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(ds, training_data: bool, eng_str=\"en\", fr_str=\"fr\", ):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        en_tokenizer (_type_): _description_\n",
    "        fr_tokenizer (_type_): _description_\n",
    "        eng_str (str, optional): _description_. Defaults to \"en\".\n",
    "        fr_str (str, optional): _description_. Defaults to \"fr\".\n",
    "        data_pd (_type_, optional): _description_. Defaults to None.\n",
    "        index_output (bool, optional): _description_cuda. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # initialize the language classes and get the data pairs (English, France)\n",
    "    en_lang, fr_lang, data_pairs = read_lang(eng_str=eng_str, fr_str=fr_str, ds=ds) # Initialize language objects\n",
    "    if training_data:\n",
    "        print(\"Adding training set sentences to Langs amd geting data pairs...\")\n",
    "        for i in tqdm(range(len(data_pairs))): # create language dictionaries\n",
    "            en_lang.addSentence(data_pairs[i][0].lower(), en_tokenizer, fr_tokenizer)\n",
    "            fr_lang.addSentence(data_pairs[i][1].lower(), en_tokenizer, fr_tokenizer)\n",
    "        # organize the langs\n",
    "        print(\"Organizing Langs...\")\n",
    "        en_lang = lang_organizer(en_lang)\n",
    "        fr_lang = lang_organizer(fr_lang)\n",
    "        # save the langs into pickle files\n",
    "        with open(f'data/en_lang_abridged_90.pickle', 'wb') as handle:\n",
    "            pickle.dump(en_lang, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(f'data/fr_lang_abridged_90.pickle', 'wb') as handle:\n",
    "            pickle.dump(fr_lang, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        print(\"Adding validations set sentences, so importing training set dictionaries...\")\n",
    "        with open(f'data/en_lang_abridged_90.pickle', 'rb') as handle:\n",
    "            en_lang = pickle.load(handle)\n",
    "        with open(f'data/fr_lang_abridged_90.pickle', 'rb') as handle:\n",
    "            fr_lang = pickle.load(handle)\n",
    "\n",
    "    print(\"Converting strings to tokens...\")\n",
    "    data_pairs = string_data_to_tokens(data_pairs,en_lang, fr_lang,\"data/tokenized_test\") # converts sequence to tokens\n",
    "    print(\"Done Converting\")\n",
    "    #  return en_lang, fr_lang, data_pairs\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the dataframe and storing untokenized pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2252039/2252039 [00:17<00:00, 131452.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding validations set sentences, so importing training set dictionaries...\n",
      "Converting strings to tokens...\n",
      "Creating tokenized pairs of english and french sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2252039/2252039 [04:37<00:00, 8107.63it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Converting\n"
     ]
    }
   ],
   "source": [
    "ds = pd.read_csv(\"data/full_data_set_splits/en-fr-test.csv\",encoding=\"utf-8\", keep_default_na=False)\n",
    "data_preprocessing(ds,training_data= False, eng_str=\"en\", fr_str=\"fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader_full import *\n",
    "\n",
    "test_ds = Test_dataset(\"data/tokenized2_en.csv\",\"data/tokenized2_fr.csv\",\"data/en_lang_90.pickle\", \"data/fr_lang_90.pickle\", sequence_length=100)\n",
    "\n",
    "dataloader = DataLoader(test_ds, batch_size=8, shuffle=False, num_workers=6)\n",
    "\n",
    "# f_transformer_test is the transformer\n",
    "\n",
    "def inference(model, src_data, tgt_data):\n",
    "    model.eval()\n",
    "    batch_size = src_data.shape[0]\n",
    "    # initialize start of sentence\n",
    "    y_init = torch.LongTensor([CustomTokens.SOS.value]).unsqueeze(0).cuda().view(1, 1)\n",
    "    y_init = y_init.repeat(batch_size,1)\n",
    "\n",
    "    # generate output positional encoding\n",
    "    toy_embeddings = f_transformer_test.decoder_embedding(tgt_data)\n",
    "    output_encoding_for_inference = f_transformer_test.positional_encoding(toy_embeddings)\n",
    "\n",
    "    # generate the mask for decoder\n",
    "    _,tgt_mask = f_transformer_test.generate_mask(inputs, outputs)\n",
    "\n",
    "    # generate the encoder output from the encoder\n",
    "    _, encoder_output = f_transformer_test(src_data, tgt_data)\n",
    "\n",
    "    # inferencing\n",
    "    for i in range(out_seq_len - 1):\n",
    "        # get the embedding of the decoder input\n",
    "        inf_emb = f_transformer_test.decoder_embedding(y_init)\n",
    "        # added up with the positional encoding\n",
    "        output_encoding_for_inference[:,:y_init.shape[1],:] = inf_emb + output_encoding_for_inference[:,:y_init.shape[1],:]\n",
    "        # get the decoder output and the probabilities of all the values\n",
    "        decoder_output = f_transformer_test.pass_through_decoder(output_encoding_for_inference, encoder_output, tgt_mask)\n",
    "        decoder_output = f_transformer_test.fc(decoder_output)\n",
    "        # get the final word with highest probabilities\n",
    "        _, next_word = torch.max(\n",
    "                decoder_output[:, y_init.shape[1] - 1 : y_init.shape[1],:], dim=2\n",
    "            )\n",
    "        # generate the final output\n",
    "        y_init = torch.cat([y_init, next_word.view(32,1)], dim=1)\n",
    "\n",
    "\n",
    "    ## this part i haven't finish\n",
    "    # convert original input from list to tokens\n",
    "    # input_test = data_en.list_of_tokens_to_list_of_words(torch.squeeze(inputs), data_en.en_lang)\n",
    "    # convert output from list to tokens\n",
    "    # output_test = data_en.list_of_tokens_to_list_of_words(torch.squeeze(y_init), data_en.fr_lang)\n",
    "    # convert output ground truth from list to tokens\n",
    "    # output_real = data_en.list_of_tokens_to_list_of_words(torch.squeeze(outputs), data_en.fr_lang)\n",
    "\n",
    "        \n",
    "    return input_test, output_test, output_real"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
