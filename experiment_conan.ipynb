{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import copy\n",
    "import spacy\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "from funnel_transformer_conan import *\n",
    "from data_loader import *\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/funnel_experiment2')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnFrDataset2(Dataset):\n",
    "    def __init__(self, Generate_train_test_split:bool):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            abridged (bool): Use the generated abridged dataset \n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        self.Generate_train_test_split = Generate_train_test_split\n",
    "        self.full_dataset_path = Path(\"data/en-fr.csv\")\n",
    "        self.abridged_dataset_path = Path(\"data/en-fr-training.csv\")\n",
    "        \n",
    "        self.en_tokenizer = get_tokenizer(tokenizer='spacy',language='en_core_web_sm')\n",
    "        self.fr_tokenizer = get_tokenizer(tokenizer='spacy',language='fr_core_news_sm')\n",
    "        \n",
    "        self.process()\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "         # Create abridged dataset if it doesnt exist and load either full or abridged data into self.ds \n",
    "        # full_dataset_path = 'data/en-fr.csv'\n",
    "        # abridged_dataset_path = 'data/en-fr-abridged.csv'\n",
    "        self.full_dataset_path.parent.mkdir(parents=True, exist_ok=True) # make datafolder if it doesn't exist\n",
    "        \n",
    "        # Check if the full dataset exists\n",
    "        if not self.full_dataset_path.exists():\n",
    "            raise FileNotFoundError(\"The full dataset does not exist. Please download it from https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset/data and place in the /data folder.\")\n",
    "        \n",
    "        # Create the abridged dataset if it does not exist\n",
    "        if  self.Generate_train_test_split:      \n",
    "            print(\"Creating abridged dataset...\")\n",
    "            print(\"reading full dataset...\")\n",
    "            full_dataset = pd.read_csv(self.full_dataset_path)\n",
    "            print(len(full_dataset))\n",
    "            training_len = int(len(full_dataset)*0.9)\n",
    "            print((training_len))\n",
    "            testing_len = int(len(full_dataset)-training_len)\n",
    "            print((testing_len))\n",
    "            print(\"Creating abridged back dataset train...\")\n",
    "            abridged_dataset = full_dataset.head(training_len)\n",
    "            print(\"Creating abridged back dataset test...\")\n",
    "            abridged_dataset_back = full_dataset[training_len:]\n",
    "            del full_dataset\n",
    "            print(\"Creating abridged dataset...\")\n",
    "            abridged_dataset.to_csv(self.abridged_dataset_path, index=False)\n",
    "            abridged_dataset_back.to_csv(\"data/en-fr-testing.csv\", index=False)\n",
    "\n",
    "        \n",
    "        # self.ds = pd.read_csv(self.abridged_dataset_path, encoding=\"utf-8\", keep_default_na=False) if self.use_abridged_data else pd.read_csv(self.full_dataset_path, encoding=\"utf-8\", keep_default_na=False)\n",
    "        \n",
    "        # self._data_preprocessing() # create data_pairs of lists of tokens\n",
    "        pass\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data_pairs[idx][0]), torch.tensor(self.data_pairs[idx][1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = get_tokenizer(tokenizer='spacy',language='en_core_web_sm')\n",
    "fr_tokenizer = get_tokenizer(tokenizer='spacy',language='fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = EnFrDataset2(Generate_train_test_split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lang(ds, eng_str=\"en\", fr_str=\"fr\"):\n",
    "    # return if the dataformat is wrong\n",
    "    if type(ds) != pd.core.frame.DataFrame:\n",
    "        raise TypeError(\"Wrong dataframe format!\")\n",
    "        \n",
    "    print(\"Reading the dataframe and storing untokenized pairs...\")\n",
    "    pairs = [(ds[eng_str][i], ds[fr_str][i]) for i in tqdm(range(len(ds)))]\n",
    "    \n",
    "    # create the class objects of Langs for English and French to count the \n",
    "    eng_lang = Langs(\"en\")\n",
    "    fr_lang = Langs(\"fr\")\n",
    "    return eng_lang, fr_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1000\n",
    "def string_to_token_list(sequence, lang):\n",
    "        \"\"\"Tokenize a sequence string in the given english/french language and return the list of tokens.\n",
    "\n",
    "        Args:\n",
    "            sequence (string): _description_\n",
    "            lang (_type_): _description_\n",
    "            en_tokenizer (_type_): _description_\n",
    "            fr_tokenizer (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "\n",
    "        token_list = []\n",
    "        if lang.lang == \"en\":\n",
    "            words = en_tokenizer(sequence.lower())\n",
    "        else:\n",
    "            words = fr_tokenizer(sequence.lower())\n",
    "            \n",
    "        # truncate the word list if it exceeds the max allowed sequence length\n",
    "        words = words[:max_seq_length - 2] # -2 is to account for the appended SOS and EOS token\n",
    "        \n",
    "        token_list.append(CustomTokens.SOS.value)\n",
    "        for word in words:\n",
    "            if word in lang.word2index:\n",
    "                token_list.append(lang.word2index[word])\n",
    "            else:\n",
    "                token_list.append(CustomTokens.UNK.value)\n",
    "        \n",
    "        token_list.append(CustomTokens.EOS.value)\n",
    "        \n",
    "        # # pad the remainder of the token list \n",
    "        # while len(token_list) < max_seq_length:\n",
    "        #     token_list.append(CustomTokens.PAD.value)\n",
    "        \n",
    "        return token_list\n",
    "\n",
    "def string_data_to_tokens(data, en_lang, fr_lang, filename):\n",
    "    \"\"\"Create tokenized pairs of english and french sentences\n",
    "\n",
    "    Args:\n",
    "        data (_type_): Dictionary of english and french sentences\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    tokenized_data = []\n",
    "    fr_string = \"_fr.csv\"\n",
    "    en_string = \"_en.csv\"\n",
    "    print(\"Creating tokenized pairs of english and french sentences...\")\n",
    "    \n",
    "    with open(filename+en_string, 'w') as csvfile1, open(filename+fr_string, 'w') as csvfile2:  \n",
    "        # creating a csv writer object  \n",
    "        csvwriter1 = csv.writer(csvfile1)  \n",
    "        csvwriter2 = csv.writer(csvfile2)\n",
    "        for i in tqdm(range(len(data))):\n",
    "        # writing the fields  \n",
    "            csvwriter1.writerow(string_to_token_list(data[i][0], en_lang))  \n",
    "            csvwriter2.writerow(string_to_token_list(data[i][1], fr_lang))  \n",
    "\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(ds, eng_str=\"en\", fr_str=\"fr\"):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        en_tokenizer (_type_): _description_\n",
    "        fr_tokenizer (_type_): _description_\n",
    "        eng_str (str, optional): _description_. Defaults to \"en\".\n",
    "        fr_str (str, optional): _description_. Defaults to \"fr\".\n",
    "        data_pd (_type_, optional): _description_. Defaults to None.\n",
    "        index_output (bool, optional): _description_cuda. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # initialize the language classes and get the data pairs (English, France)\n",
    "    en_lang, fr_lang, data_pairs = read_lang(eng_str=eng_str, fr_str=fr_str, ds=ds) # Initialize language objects\n",
    "    print(\"Adding sentences to Langs amd geting data pairs...\")\n",
    "    # for i in tqdm(range(len(data_pairs))): # create language dictionaries\n",
    "    #     en_lang.addSentence(data_pairs[i][0].lower(), en_tokenizer, fr_tokenizer)\n",
    "    #     fr_lang.addSentence(data_pairs[i][1].lower(), en_tokenizer, fr_tokenizer)\n",
    "    print(\"generating the pickle files for dictionary\")\n",
    "    with open(f'data/en_lang_90.pickle', 'rb') as handle:\n",
    "        en_lang = pickle.load(handle)\n",
    "    with open(f'data/fr_lang_90.pickle', 'rb') as handle:\n",
    "        fr_lang = pickle.load(handle)\n",
    "    data_pairs = string_data_to_tokens(data_pairs,en_lang, fr_lang,\"data/tokenized_test\") # converts sequence to tokens\n",
    "    print(\"Done Converting\")\n",
    "    #  return en_lang, fr_lang, data_pairs\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"data/en-fr-training.csv\")\n",
    "data_preprocessing(ds, eng_str=\"en\", fr_str=\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader_full import *\n",
    "\n",
    "test_ds = Test_dataset(\"data/tokenized2_en.csv\",\"data/tokenized2_fr.csv\",\"data/en_lang_90.pickle\", \"data/fr_lang_90.pickle\", sequence_length=100)\n",
    "\n",
    "dataloader = DataLoader(test_ds, batch_size=8, shuffle=False, num_workers=6)\n",
    "\n",
    "# f_transformer_test is the transformer\n",
    "\n",
    "def inference(model, src_data, tgt_data):\n",
    "    model.eval()\n",
    "    batch_size = src_data.shape[0]\n",
    "    # initialize start of sentence\n",
    "    y_init = torch.LongTensor([CustomTokens.SOS.value]).unsqueeze(0).cuda().view(1, 1)\n",
    "    y_init = y_init.repeat(batch_size,1)\n",
    "\n",
    "    # generate output positional encoding\n",
    "    toy_embeddings = f_transformer_test.decoder_embedding(tgt_data)\n",
    "    output_encoding_for_inference = f_transformer_test.positional_encoding(toy_embeddings)\n",
    "\n",
    "    # generate the mask for decoder\n",
    "    _,tgt_mask = f_transformer_test.generate_mask(inputs, outputs)\n",
    "\n",
    "    # generate the encoder output from the encoder\n",
    "    _, encoder_output = f_transformer_test(src_data, tgt_data)\n",
    "\n",
    "    # inferencing\n",
    "    for i in range(out_seq_len - 1):\n",
    "        # get the embedding of the decoder input\n",
    "        inf_emb = f_transformer_test.decoder_embedding(y_init)\n",
    "        # added up with the positional encoding\n",
    "        output_encoding_for_inference[:,:y_init.shape[1],:] = inf_emb + output_encoding_for_inference[:,:y_init.shape[1],:]\n",
    "        # get the decoder output and the probabilities of all the values\n",
    "        decoder_output = f_transformer_test.pass_through_decoder(output_encoding_for_inference, encoder_output, tgt_mask)\n",
    "        decoder_output = f_transformer_test.fc(decoder_output)\n",
    "        # get the final word with highest probabilities\n",
    "        _, next_word = torch.max(\n",
    "                decoder_output[:, y_init.shape[1] - 1 : y_init.shape[1],:], dim=2\n",
    "            )\n",
    "        # generate the final output\n",
    "        y_init = torch.cat([y_init, next_word.view(32,1)], dim=1)\n",
    "\n",
    "\n",
    "    ## this part i haven't finish\n",
    "    # convert original input from list to tokens\n",
    "    # input_test = data_en.list_of_tokens_to_list_of_words(torch.squeeze(inputs), data_en.en_lang)\n",
    "    # convert output from list to tokens\n",
    "    # output_test = data_en.list_of_tokens_to_list_of_words(torch.squeeze(y_init), data_en.fr_lang)\n",
    "    # convert output ground truth from list to tokens\n",
    "    # output_real = data_en.list_of_tokens_to_list_of_words(torch.squeeze(outputs), data_en.fr_lang)\n",
    "\n",
    "        \n",
    "    return input_test, output_test, output_real"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
