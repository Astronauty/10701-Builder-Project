{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import copy\n",
    "import spacy\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from funnel_transformer_conan import *\n",
    "from data_loader import *\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/funnel_experiment2')\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the dataframe and storing untokenized pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 163147.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding sentences to Langs amd geting data pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 6349.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tokenized pairs of english and french sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 12229.87it/s]\n"
     ]
    }
   ],
   "source": [
    "data_en = EnFrDataset(used_abridged_data=True, max_seq_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(data_en, batch_size=32, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "dafaq",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/aipex6/Conan/10701-Builder-Project/funnel_transformer_conan_2.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aipex6/Conan/10701-Builder-Project/funnel_transformer_conan_2.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m100\u001b[39m\u001b[39m%\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoder_blocks)\u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aipex6/Conan/10701-Builder-Project/funnel_transformer_conan_2.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m100\u001b[39m\u001b[39m/\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoder_blocks)\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/aipex6/Conan/10701-Builder-Project/funnel_transformer_conan_2.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39massert\u001b[39;00m(\u001b[39m100\u001b[39m\u001b[39m%\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoder_blocks)\u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39m100\u001b[39m\u001b[39m/\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoder_blocks)\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mdafaq\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aipex6/Conan/10701-Builder-Project/funnel_transformer_conan_2.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m decoder_blocks \u001b[39m=\u001b[39m \u001b[39m6\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aipex6/Conan/10701-Builder-Project/funnel_transformer_conan_2.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m dropout \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: dafaq"
     ]
    }
   ],
   "source": [
    "from data_loader import *\n",
    "from funnel_transformer_conan import *\n",
    "# full_dataset_path = Path('data/en-fr.csv')\n",
    "# abridged_dataset_path = Path('data/en-fr-abridged.csv')\n",
    "\n",
    "# ds = pd.read_csv(abridged_dataset_path)\n",
    "# english, french, pairs = read_lang('en', 'fr', abridged_dataset)\n",
    "\n",
    "# src_vocab_size = 5000\n",
    "# tgt_vocab_size = 5000\n",
    "# d_model = 512\n",
    "# num_heads = 8\n",
    "# num_layers = 6\n",
    "# d_ff = 2048\n",
    "# max_seq_length = 100\n",
    "# dropout = 0.1\n",
    "\n",
    "src_vocab_size = data_en.get_src_lang_size()\n",
    "tgt_vocab_size = data_en.get_tgt_lang_size()\n",
    "d_model = 512\n",
    "num_heads = 4\n",
    "d_ff = 2014\n",
    "max_seq_length = 100\n",
    "encoder_blocks = 3\n",
    "print(100%(2**encoder_blocks)!=0)\n",
    "print(100/(2**encoder_blocks)==0)\n",
    "assert(100%(2**(encoder_blocks-1))!=0 or 100/(2**(encoder_blocks-1))==0)==0,\"dafaq\"\n",
    "decoder_blocks = 6\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Funnel_Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, decoder_blocks, encoder_blocks, d_ff, max_seq_length, dropout)\n",
    "# # Generate random sample data\n",
    "# src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "# tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "\n",
    "\n",
    "\n",
    "loss_criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "batch_size = 64\n",
    "Funnel_Transformer = Funnel_Transformer.to(device)\n",
    "total_loss = 0.0\n",
    "for epoch in range(1000):\n",
    "    epoch_loss = []\n",
    "    transformer.train()\n",
    "    for it in train_dataloader:\n",
    "        src_data, tgt_data = it\n",
    "        src_data = src_data.to(device)\n",
    "        tgt_data = tgt_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output= transformer(src_data, tgt_data[:, :-1])\n",
    "        break\n",
    "        loss = loss_criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    epoch_loss.append(loss.item())\n",
    "    writer.add_scalar('training loss',\n",
    "                    total_loss,\n",
    "                        epoch)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch: {epoch+1}, Epoch Loss: {total_loss}\")\n",
    "    total_loss = 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (139525130.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    a = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout),nn.MaxPool1d(3,stride=2) for _ in range(num_layers)])\u001b[0m\n\u001b[0m                                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout),nn.MaxPool1d(3,stride=2) for _ in range(num_layers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9, 5, 4,  ..., 6, 7, 4],\n",
      "        [8, 9, 3,  ..., 6, 8, 8],\n",
      "        [8, 3, 2,  ..., 6, 7, 6],\n",
      "        ...,\n",
      "        [6, 9, 6,  ..., 8, 2, 6],\n",
      "        [6, 7, 9,  ..., 8, 1, 4],\n",
      "        [3, 9, 2,  ..., 4, 1, 3]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.randint(1,10,(64,100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
